{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75a77a7",
   "metadata": {},
   "source": [
    "# üîê AI-Powered Data Breach Detection\n",
    "This notebook is a personal/public log book which helps me/you to understand the entire working of this project step by step ‚Äî from data preprocessing, model training, to Streamlit deployment with Docker, etc.\n",
    "\n",
    "WHY?\n",
    "So that I personally can better understand and keep a check of the things that I did on this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c838adc",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Why this Project?\n",
    "While scrolling through LinkedIn one day, I came across a news article reporting a major security breach involving the leakage of sensitive information of millions of users. This incident sparked a question in my mind:\n",
    "\"What if I could help detect such malicious activities even before the damage is done using machine learning?\"\n",
    "\n",
    "At the time, I was going through the Machine Learning Specialization by Stanford & DeepLearning.AI, where I learned several techniques for building ML models based on different types of data and use cases. That‚Äôs when I decided to work on a project that not only addresses a real-world problem but also help me land an internship.\n",
    "\n",
    "After brainstorming ideas‚Äîboth on my own and with the help of ChatGPT, I landed on the concept of an AI/ML-Powered Data Breach Detection System:\n",
    "A system that analyzes network traffic to detect suspicious patterns and flag them as potential security threats.\n",
    "\n",
    "This project is my first major step in combining Machine Learning with Cybersecurity, and it has become one of my most impactful beginner-friendly projects so far for me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da294b79",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£  Project Overview\n",
    "**Dataset:** UNSW-NB15\n",
    "* I have used a real world publicly available dataset created by the IXIA PerfectStorm tool in the Cyber Range Lab of UNSW Canberra for generating a hybrid of real modern normal activities and synthetic contemporary attack behaviours. This dataset constist of nine types of attacks such as Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode and Worms.\n",
    "\n",
    "* This dataset simulates how network traffic appears in real life while containing features like Source IP, Destination IP, Ports, Protocol, etc.\n",
    "\n",
    "* WHY?\n",
    "It is realistic and mordern, while having rich features and plenty types of attacks.\n",
    "\n",
    "**Goal:** Predict whether a network traffic instance is malicious or normal\n",
    "* This project is the first deployed project I ever created hence I wanted it to be effective yet simple thus classifying the network traffic into malicious or normal type.\n",
    "\n",
    "* The basic goal was to analyze the incomming traffic features for any suspected activities or irregular network data using machine learning models. \n",
    "\n",
    "**Models Used:** Random Forest, XGBoost, MLPClassifier\n",
    "\n",
    "* Now you might ask that why I used three different models?\n",
    "Why not?\n",
    "When I explore Linkedin, I often see very basic projects made using Machine Learning and I did not preferred to do that. \n",
    "As I have learned about RF, XGB, MLP in my courses why not use all these to train my models to compare who fast and efficient are they in my use case.\n",
    "\n",
    "* I wanted to create and know how these models differ in working and training and results.\n",
    "\n",
    "**Deployment:** Streamlit\n",
    "\n",
    "* Currently only deployed on streamlit but will soon also deploy uding docker.\n",
    "\n",
    "WHY?\n",
    "Streamlit is very easy.\n",
    "Used AI to implement the frontend as I know very little about the Frontend Development [Soon I will master it too :)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30991152",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c8114",
   "metadata": {},
   "source": [
    "## Now step by step we will discuss my entire project in details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e2625",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Preprocessing\n",
    "The first step of building this project was to deeply explore and understand the data set I had.\n",
    "This was done using a Jupyter notebook called `01_data_eda.ipynb` in Notebooks folder.\n",
    "\n",
    "‚û°Ô∏è Goals using EDA:\n",
    "1. Understanding the rows, coloums and data types.\n",
    "2. Identifying any **missing values**, **duplicate rows**, and **non-numeric** features.\n",
    "3. Preparing the dataset for preprocessing and modeling.\n",
    "\n",
    "‚û°Ô∏èWhat did I do in the EDA?\n",
    "1. **Loaded the Dataset**\n",
    "   - Combined `UNSW-NB15_1.csv` and `UNSW-NB15_2.csv` using `pd.concat()`.\n",
    "\n",
    "2. **Initial Exploration**\n",
    "   - Used `df.info()` and `df.describe()` to understand structure and summary stats.\n",
    "   - Checked number of `NaNs`, `nulls`, and `duplicates`.\n",
    "\n",
    "3. **Label Analysis**\n",
    "   - Counted the number of Normal (0) vs Malicious (1) rows.\n",
    "   - Found the dataset to be **imbalanced**.\n",
    "   - Here there were more **Normal** rows than **Malicious** rows.\n",
    "\n",
    "4. **Attack Category Breakdown**\n",
    "   - Explored the `attack_cat` column to see which attacks are most common.\n",
    "   - Example: Reconnaissance, Exploits, Fuzzers were the top categories.\n",
    "\n",
    "5. **Feature Types**\n",
    "   - Identified **categorical** vs **numerical** features.\n",
    "   - Noted key categorical features: `proto`, `service`, `state`.\n",
    "\n",
    "6. **Decision**\n",
    "   - Removed columns like `srcip`, `sport`, `dstip`, `dsport` (non-useful for our prediction).\n",
    "   - Marked `proto`, `service`, and `state` for encoding.\n",
    "   - Planned feature selection based on correlation and missing values.\n",
    "\n",
    "\n",
    "‚û°Ô∏èWhy was EDA important?\n",
    "- To find Dirty/messy columns\n",
    "- To find Redundant or identifier columns\n",
    "- To figure out Imbalanced label distribution\n",
    "- To figure out the exact steps needed in the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514933b",
   "metadata": {},
   "source": [
    "In the same notebook I have also trained my model using RF, which I will explain in next cell.\n",
    "\n",
    "After completing EDA, it was time to create our 'data_processing.py' file where I performed loading, cleaning, encoding and splitting of the data set.\n",
    "\n",
    "It ensures the same work as out notebook file just in more of a modular way, hence we will directly discuss our model file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646ffda",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Model Training and Evaluation\n",
    "Now that I have completed the data cleaning, encoding, and class balancing, the next step is to train a machine learning model.\n",
    "\n",
    "I used a **Random Forest Classifier** ‚Äì an ensemble learning method that combines multiple decision trees and is known for its performance and robustness in handling structured data like this network traffic dataset.\n",
    "\n",
    "### Goals:\n",
    "1. Split the dataset into training and testing sets.\n",
    "2. Balance the training data using SMOTE.\n",
    "3. Train a Random Forest Classifier on the balanced data.\n",
    "4. Evaluate the model using accuracy, precision, recall, and F1-score.\n",
    "5. Save the trained model, encoders, and feature columns for deployment.\n",
    "\n",
    "The file 'src/Model.py' has the entire logic and model training for my project using RF.\n",
    "\n",
    "Importing major libraries such as:- \n",
    "1. Pandas / NumPy: Data manipulation.\n",
    "2. RandomForestClassifier: For machine learning algorithm.\n",
    "3. classification_report / confusion_matrix: To evaluate the model's performance.\n",
    "4. joblib: For saving models and encoders.\n",
    "5. SMOTE: To handle class imbalance.\n",
    "\n",
    "* Then I defined the coloum names that are needed to train the model where I later loaded the datasets UNSW-NB15_1 and UNSW-NB15_2 and processed the data using imported custom modules like load_data, preprocess_dataframe, and split_and_clean from our data_processing.py file to modularize the pipeline.\n",
    "\n",
    "* Later I filled all the missing values with coloum's median and empty spaces as NaN.\n",
    "\n",
    "* Then the data was split for training and testing the model into two sets **X_train**, **Y_train**, **X_test**, **Y_test** also seprating data labels X & Y.\n",
    "\n",
    "* Next I applied SMOTE (Synthetic Minority Oversampling) that generates synthetic samples to balance the datasets which was required in my case for good model training.\n",
    "\n",
    "* Then it was time to train the model using RandomForestClassifier with 100 decision trees (estimators).\n",
    "\n",
    "* Then I saved the feature coloums order and names that were used during the training so that we can process the new data in the same manner.\n",
    "\n",
    "* The last part of this model file was to evaluate our model, to make prediction on the test set and showing how good our model performs using accuracy, precision , recall, and F1 score.\n",
    "These evaluation was included in our Notebook where Confusion Matrix, Heatmap, and Classification Report was presented well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1b2a8",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Inference\n",
    "It's time to create a new file called 'predict.py' now, which will be used to run our model on newer sets of data given by us to predict where the traffic is malicious or normal.\n",
    "\n",
    "* Objective\n",
    "- Load the saved model, encoders, and training feature columns.\n",
    "- Preprocess a new CSV file or a dummy sample input.\n",
    "- Generate predictions for whether the traffic is **Normal** or **Malicious**.\n",
    "- Save the results with prediction labels and confidence scores.\n",
    "\n",
    "The input can be a CSV file with data or else if there is not CSV file uploaded then I have set few default values which will be used to run the model just for the sake of demonstration.\n",
    "\n",
    "* Understanding the Prediction Pipeline\n",
    "The prediction process includes the following steps:\n",
    "1. **Loading model assets**: RandomForest model, label encoders, and feature columns used in training.\n",
    "2. **Reading new input data**: Either from a user-uploaded CSV or a default dummy traffic sample.\n",
    "3. **Preprocessing data**: Ensuring input data is formatted exactly as expected (just like our training data).\n",
    "4. **Making predictions**: Using the model to classify each input row as 'Normal' or 'Malicious'.\n",
    "5. **Saving predictions**: Results are saved to `prediction_output.csv`.\n",
    "\n",
    "We maintain the same pipeline used in training to ensure same model consistency and to avoid any input mismatch errors.\n",
    "\n",
    "* Conclusion\n",
    "\n",
    "We successfully:\n",
    "- Loaded our trained ML model and encoders onto the file\n",
    "- Preprocessed our new network traffic data\n",
    "- Predicted whether the traffic is Normal or Malicious\n",
    "- Saved the results for further analysis\n",
    "\n",
    "This forms our final **prediction pipeline** which is ready for production or integration into dashboards like steamlit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8640c2",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Multiple Model Training\n",
    "We will now train two other models with different methods of MLPClassifier and XGBoost.\n",
    "We will compute and compare:\n",
    "- üéØ Random Forest\n",
    "- üöÄ XGBoost\n",
    "- üß† MLPClassifier (Neural Network)\n",
    "\n",
    "We aim to:\n",
    "- Train each model on the UNSW-NB15 dataset.\n",
    "- Evaluate and compare their performance using metrics like Accuracy, F1 Score, and Confusion Matrix.\n",
    "- Visualize results and perform SHAP-based model explainability.\n",
    "- Save all models and metrics for use in prediction/deployment.\n",
    "\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "We successfully trained and evaluated three ML models:\n",
    "- üå≤ Random Forest\n",
    "- üî• XGBoost\n",
    "- üß† MLP Neural Net\n",
    "\n",
    "**XGBoost** and **Random Forest** performed with very high accuracy and F1-score but XGBoost had the least amount of time required to train thus being the best for our required case.\n",
    "\n",
    "We also:\n",
    "- Visualized metrics\n",
    "- Interpreted predictions using SHAP\n",
    "- Saved all trained models and metrics for deployment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ef5b1",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ SHAP Explainability\n",
    "* What is SHAP Explainability?\n",
    "SHAP (SHapley Additive exPlanations) is a technique to explain how your machine learning model makes decisions.\n",
    "Basically,\n",
    "SHAP explains like which features (like Sload, dbytes, state, etc.) influenced our model to predict a sample as malicious or normal ‚Äî and by how much?\n",
    "\n",
    "* WHY SHAP?\n",
    "- Helps to understand that which features are most useful\n",
    "- Increases the ability to debug our model\n",
    "- Provides better understanding of the model\n",
    "\n",
    "It is available on our Streamlit Dashboard as well as our second notebook where we performed advance EDA and SHAP Explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05a292",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Streamlit & App Logic\n",
    "What is Streamlit: \n",
    "Streamlit is an open-source Python library that helps ud create interactive web apps for any projects ‚Äî using just Python!\n",
    "\n",
    "üîπ With Streamlit, we don't need to learn HTML/CSS/JavaScript.\n",
    "üîπ We just write Python scripts, and Streamlit handles the UI.\n",
    "\n",
    "Streamlit was an easy option for me as I had no idea how would I make a dashboard.\n",
    "Hence CHATGPT helped me a lot creating the entire frontend for me, making this project beautifully possible.\n",
    "\n",
    "üîπ How to use my APP?\n",
    "- Upload CSV file\n",
    "- Choose any one of the model from the sidebar\n",
    "- Visualize the predictions (bar, pie, histogram)\n",
    "- Shows SHAP plot and performance metrics\n",
    "- Download the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d783fa",
   "metadata": {},
   "source": [
    "## Deployment on Streamlit Cloud\n",
    "- GitHub Repo: https://github.com/IbhavMalviya/AI-Data-Breach-System\n",
    "- Streamlit detects `App/dashboard.py`\n",
    "- App runs on: https://ai-data-breach-system.streamlit.app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ea9e5",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Final Thoughts\n",
    "It was surely a exciting project to make. From knowing nothing about machine learning to actually building a project from the scratch.\n",
    "The internet helped me a lot to know things which I was afraid to learn before.\n",
    "It surely took time for me to make this project but it was all worth it.\n",
    "Well, Onto the next adventure of the new project I guess.\n",
    "See you soon :)\n",
    "---\n",
    "### Project by: **Ibhav Malviya**\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-blue?logo=linkedin)](https://linkedin.com/in/ibhavmalviya)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-black?logo=github)](https://github.com/IbhavMalviya)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
